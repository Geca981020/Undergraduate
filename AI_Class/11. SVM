Chapter 11 SVM(Support Vector Machines)
11. 1 Introduction
- Linear Separators
데이터를 분리할 hyperplane을 찾기.
어떤 hyperplane이 가장 optimal 한가
margin으로 정의하며 margin이 가장 큰 hyperplane이 제일 좋다.

- r은 example x와 linear Separator까지의 거리.
- support vector는 hyperplane에 가장 가까운 example 
- margin은 분리기의 클래스간의 분리폭 이다.

- 마진을 최대화하기 위해서 PAC(Probably Approximately Correct)이론을 사용
support vector만 중요함을 의미. 다른 training example을 무시할 수 있다.

11. 2 Linear SVM Mathematically
- 모든 데이터가 최소 margin 1이라고 가정.

- margin을 최대화 할 수 있는 w와 b를 찾는다.

- 2번째를 사용해서 Learning을 수행.
- Quadratic Programming: 최적화 문제
여러 변수의 quadratic function을 최적화하는 문제(최대화, 최소화)는 이러한 변수의 선형제약에 따라 결정된다. / standard Lagrange multiplier Method를 사용.

11. 3 standard Lagrange multiplier Method
- Equality Constraints (SVM에서 사용 X)
- Inequality Constraints(SVM에서 사용 O)
KKT Conditions에 주목

11. 4 margin에 standard Lagrange multiplier Method적용
- 해결을 위해 다음 함수를 고려.
- KKT Conditions

- 위 문제는 Lagrange multipliers를 Lagrangian function으로 변환함으로써 단순화 할 수 있다(dual problem).


- 학습 데이터마다 람다가 1개씩 나오게 된다.
- 0이 아닌 람다는 해당 x가 support vector임을 말한다.

- 분류함수의 형식
-> test point x와 support vector의 x사이에 내적을 수행
   벡터간의 유사도를 볼 때 내적을 사용.
   또한 최적화 문제를 해결하려면 모든 training point사이의 내적을 계산해야함.

11. 5 Soft Margin Classification
- 만약 training set이 선형적으로 분류되지 않으면 어떻게 해야할까
  Slack Variable ɛ을 추가하여 분류하기 어려운 예제를 분류한다. 
  Slack Variable 은 최소화, Margin은 최대화

- k는 hyperparameter이다.

11. 6 Non Linear SVM
- data set은 고차원 공간으로 보낼 때 어떤 공간에서 Linear Separator할 수 있다.
하지만 어떤 공간인지는 알 수 없다.

- 파이간의 곱은 inner product이며, 고차원으로 보내고 고차원에서 내적을 구한다는 것을 의미.
- Kernel Trick: K(x, x) = Φ(x)*Φ(x)
- Kernel Function: 고차원 공간에서 내적으로 값을 구한 것과 같다.
  커널 공간에서 입력 값의 유사성 측정.
  고차원 공간 내적에 할당.
- 커널만 계산하면 된다. 굳이 고차원으로 데이터를 보낼 필요도, 내적할 필요도 없다.

- 커널 기반 학습 방법
1. 일시적으로 데이터를 고차원 공간에 매핑
2. 해당 공간에서 간단한 학습 알고리즘(SVM, PCA)를 수행

- How to design the Kernal?
K(x, x) = Φ(x)*Φ(x)를 증명하기 쉽지 않으며, 전문적인 지식이 필요하다. 
- Kernel Preserving Advantage
1. semi-positive의 명확한 대칭을 증명할 필요가 없다.
2. time complexity를 쉽게 분석할 수 있다.
